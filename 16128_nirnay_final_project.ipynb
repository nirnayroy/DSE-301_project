{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase prediction in High Entropy Alloys\n"
    "\n",
    "## Abstract\n",
    "\n",
    "High Entropy Alloys(HEAs) are interesting because of useful properties like high hardness and ductility, high-temperature strength, antioxidant capacity and wear-resistance, which are absent from traditional metal alloys. These properties depend on the phase the HEAs take. HEAs can take 3 phases namely Intermetallic Compound(IM), Solid Solution(SS) and Amorphous Mixture(AM). Mixed states of IM+SS and IM+AM are also formed for some alloys. Phase prediction was done using calculations from DFT(Density Functional Theory) before. Recently, it was shown that machine learning can be used to predict phases of alloys using properties from the constituent metals like average Valence electron concentration(VEC), Pauling electronegativity, atomic size difference, mixing entropy and mixing enthalpy. In this project we analyse the data given by [Miracle and Senkov](https://www.sciencedirect.com/science/article/pii/S1359645416306759) in their review article of HEAs.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This code has the following dependencies:\n",
    "- [Numpy](https://numpy.org/)\n",
    "- [Matplotlib](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html)\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/)\n",
    "- [Keras](https://keras.io/)\n",
    "\n",
    "\n",
    "## Preparing the dataset\n",
    "\n",
    "The dataset for this analysis was forked from [ZHOU-Ziqing/MLcode-for-HEAphase](https://github.com/ZHOU-Ziqing/MLcode-for-HEAphase). The dataset has 603 HEAs and 9 features for each of them. The parameters with their corresponding formulae are given below.\n",
    "\n",
    "![alt text](parameters.png \"Features\")\n",
    "\n",
    "### Importing and Cleaning\n",
    "\n",
    "Two Alloys had Nan and infinite values. We drop them from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_file = 'hea_data.csv'\n",
    "df = pd.read_csv(data_file, encoding = \"ISO-8859-1\")\n",
    "df = pd.DataFrame(df)\n",
    "#print(df.describe)\n",
    "df = df[['Alloy', 'a', 'Tm', '?Hmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk', 'AM?', 'IM?', 'SS?']]\n",
    "#print(df.shape)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "#print(df.shape)\n",
    "#print(df.describe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "We use Anomaly Detection to find Outlier Alloys. Such Alloys may be of specific interest due to their rare properties. We use Univariate and Multivariate Anomaly Detection here.\n",
    "\n",
    "#### Interquartile Range(Univariate)\n",
    "\n",
    "Here we consider Alloys with features less or more than 1.5 times the Interquartile range of that particular feature. Interquartile range is the amount of spread in the middle 50% of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aPercentiles: 25th=1.288, 75th=1.455, IQR=0.167\n",
      "a: ['a:', 'AlB12', 'B4Co', 'AlB2', 'B6Co2Nb2', 'Sr36Al24Co20Y20', 'GdTbDyTmLu', 'Ca6.23Mg3.78Sn7', 'SrCaYbMgZn0.5Cu0.5', 'SrCaYbLi0.55Mg0.45Zn', 'Sr40Al20Co20Y20', 'SrCaYbMgZn', 'YGdTbDyLu', 'HoDyYGdTb', 'Sr46Al14Co20Y21', 'Ca65Mg15Zn20', 'GdHoLaTbY', 'Al3Ca8']\n",
      "TmPercentiles: 25th=1254.105, 75th=1851.342, IQR=597.237\n",
      "Tm: ['Tm:', 'B2CoW2', 'MoNbTaVW', 'NbMoTaW', 'HfW2']\n",
      "?HmixPercentiles: 25th=1.840, 75th=7.772, IQR=5.932\n",
      "?Hmix: ['?Hmix:', 'SmFe6Ti6N', 'Zr17Ta16Ti19Nb22Si26', 'AlMoNbSiTaTiVZr']\n",
      "Elec_negaPercentiles: 25th=1.577, 75th=1.809, IQR=0.231\n",
      "Elec_nega: ['Elec_nega:', 'Au46Ag5Cu29Si20', 'Au52Pd2.3Cu29.2Si16.5', 'AuCu', 'Pd95Si5', 'AuSb2', 'AgPt3', 'Au2Bi', 'Au9Sn', 'Au3In2', 'Al4Li9', 'AuSn', 'Au2Pb', 'AuPb2', 'Bi3Pb7', 'AuPb3', 'GdTbDyTmLu', 'SrCaYbMgZn0.5Cu0.5', 'SrCaYbLi0.55Mg0.45Zn', 'SrCaYbMgZn', 'YGdTbDyLu', 'HoDyYGdTb', 'Ca65Mg15Zn20', 'GdHoLaTbY', 'Al3Ca8']\n",
      "D_elec_negaPercentiles: 25th=0.118, 75th=0.254, IQR=0.137\n",
      "D_elec_nega: ['D_elec_nega:', 'CuGeLi2', 'HfW2']\n",
      "VECPercentiles: 25th=4.647, 75th=7.800, IQR=3.153\n",
      "VEC: ['VEC:']\n",
      "d_VECPercentiles: 25th=1.720, 75th=3.089, IQR=1.368\n",
      "d_VEC: ['d_VEC:', 'Cd5Li4Mg']\n",
      "BulkModulusPercentiles: 25th=92190000000.000, 75th=157000000000.000, IQR=64810000000.000\n",
      "BulkModulus: ['BulkModulus:', 'AlB12', 'B4Co', 'B6Co2Nb2', 'B2NiTa', 'B2Mo2Ni', 'B2CoMo2', 'B2CoW2', 'AlRe2']\n",
      "D_BulkPercentiles: 25th=27.328, 75th=49.793, IQR=22.465\n",
      "D_Bulk: ['D_Bulk:', 'AlB2', 'HfB2', 'B4Fe4Nd', 'B3FeNd2', 'B2Co3Zr', 'BCaNi4', 'AuSb2', 'AlRe2', 'HfW2', 'Au2Bi', 'Au3In2']\n"
     ]
    }
   ],
   "source": [
    "for i in ['a', 'Tm', '?Hmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk']:\n",
    "    ols = []\n",
    "    data = np.array(df[[i]])\n",
    "    #print(data)\n",
    "    ols.append(str(i)+':')\n",
    "        # calculate interquartile range\n",
    "    q25, q75 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "    iqr = q75 - q25\n",
    "    print(str(i)+'Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "    # calculate the outlier cutoff\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    for n, j in enumerate(data):\n",
    "        #print(j)\n",
    "        if j < lower or j > upper:\n",
    "            ols.append(df['Alloy'][n])\n",
    "    print(str(i)+':', ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                           |No.      | Examples                                                                 |\n",
    "| ----------------------------------| --------|--------------------------------------------------------------------------|\n",
    "| Mean Atomic Radius(a)             | 18      | 'AlB12', 'B4Co', 'AlB2', 'B6Co2Nb2', 'Sr36Al24Co20Y20', 'GdTbDyTmLu',    | |                                   |         | 'Ca6.23Mg3.78Sn7', 'SrCaYbMgZn0.5Cu0.5', 'SrCaYbLi0.55Mg0.45Zn',         | |                                   |         | 'Sr40Al20Co20Y20', 'SrCaYbMgZn', 'YGdTbDyLu', 'HoDyYGdTb',               |  |                                   |         | 'Sr46Al14Co20Y21', 'Ca65Mg15Zn20', 'GdHoLaTbY', 'Al3Ca8'                 |\n",
    "| Average Mixing Enthalpy(Hmix)      | 3       | 'SmFe6Ti6N', 'Zr17Ta16Ti19Nb22Si26', 'AlMoNbSiTaTiVZr'                   |\n",
    "| Average Melting Temperature(Tm)   | 4       | 'B2CoW2', 'MoNbTaVW', 'NbMoTaW', 'HfW2'                                  |\n",
    "| Electronegativity(E)              | 27      | 'Au46Ag5Cu29Si20', 'Au52Pd2.3Cu29.2Si16.5', 'AuCu', 'Pd95Si5','AuSb2',   | |                                   |         | 'AgPt3', 'Au2Bi', 'Au9Sn',  'Au3In2', 'Al4Li9', 'AuSn', 'Au2Pb', 'AuPb2',| |                                   |         | 'Bi3Pb7', 'AuPb3', 'GdTbDyTmLu', 'SrCaYbMgZn0.5Cu0.5',                   | |                                   |         | 'SrCaYbLi0.55Mg0.45Zn', 'SrCaYbMgZn', 'YGdTbDyLu', 'HoDyYGdTb',          |  |                                   |         | 'Ca65Mg15Zn20', 'GdHoLaTbY', 'Al3Ca8'                                    |\n",
    "| sd of Electronegativity(dE)       | 2       | 'CuGeLi2', 'HfW2'                                                        |\n",
    "| Average VEC                       | 0       |                                                                          |\n",
    "| sd of VEC                         | 1       | 'Cd5Li4Mg'                                                               |\n",
    "| Bulk modulus                      | 8       | 'AlB12', 'B4Co', 'B6Co2Nb2', 'B2NiTa', 'B2Mo2Ni', 'B2CoMo2', 'B2CoW2',   |  |                                   |         | 'AlRe2'                                                                  |\n",
    "| sd of Bulk Modulus                | 10      |  'AlB2', 'HfB2', 'B4Fe4Nd', 'B3FeNd2', 'B2Co3Zr', 'BCaNi4', 'AuSb2',     |  |                                   |         |  'AlRe2', 'HfW2', 'Au2Bi', 'Au3In2'                                      |\n",
    "\n",
    "On the basis of the above table, we can interpret the following:\n",
    "- The Mean Atomic Radius of an alloy takes on extreme values when the alloy either has Boron or Aluminium, else their are large atoms in the constituent metals.\n",
    "- The Average Mixing Enthaly takes large values in alloys of  4 or more metals.\n",
    "- Electronegativity values are extreme for 27 alloys depending on the constituents.\n",
    "- The bulk modulus takes extreme values in alloys containing Aluminium, Boron, Silver and Tungsten.\n",
    "\n",
    "####  Local Outlier Factor (Multivariate)\n",
    "\n",
    "The LOF calculates the outlier score based on local outlier factor. An anomaly score is computed by the distance of each instance to its cluster center multiplied by the instances belonging to its cluster.\n",
    "\n",
    "Using LOF we find 9 alloys which are outliers for all features. These alloys may be of further interest due to their rare feature distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the following alloys were found to be outliers\n",
      "                     Alloy         a           Tm     ?Hmix  Elec_nega  \\\n",
      "0                   AlB12  0.867077  2239.190000  0.000000   2.006923   \n",
      "1                    B4Co  0.906200  2232.000000  3.456000   2.008000   \n",
      "15                 B2CoW2  1.125000  2770.800000  7.772271   2.136000   \n",
      "339                 AlRe2  1.394000  2617.156667  0.471405   1.803333   \n",
      "481                Al4Li9  1.492231   601.314615  0.273100   1.173846   \n",
      "528              Cd5Li4Mg  1.551700   570.886000  2.709686   1.368000   \n",
      "581              CdIn3Na2  1.660333   437.535000  1.247219   1.481667   \n",
      "592  SrCaYbLi0.55Mg0.45Zn  1.755780   922.911900  6.663995   1.165700   \n",
      "\n",
      "     D_elec_nega       VEC     d_VEC   BulkModulus      D_Bulk  \n",
      "0       0.114582  3.000000  0.000000  3.012310e+11   65.018523  \n",
      "1       0.064000  4.200000  2.400000  2.920000e+11   56.000000  \n",
      "15      0.192000  5.400000  2.244994  2.880000e+11   54.184869  \n",
      "339     0.136707  5.666667  1.885618  2.720000e+11  138.592929  \n",
      "481     0.290769  1.615385  0.923077  3.100000e+10   30.000000  \n",
      "528     0.335255  6.600000  5.407402  2.990000e+10   15.456067  \n",
      "581     0.391383  3.833333  3.760171  2.576500e+10   14.101036  \n",
      "592     0.261221  4.090000  3.992731  3.123879e+10   21.863012  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "#df = df.reset_index()\n",
    "df1 = df[[ 'a', 'Tm', '?Hmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk']]\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(df1)\n",
    "# select all rows that are not outliers\n",
    "# summarize the shape of the updated training dataset\n",
    "mask = yhat == -1\n",
    "print('the following alloys were found to be outliers\\n', df[['Alloy', 'a', 'Tm', '?Hmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk']][mask])\n",
    "\n",
    "nol_df = df[yhat != -1] #df with no outliers\n",
    "nol_X = nol_df[[ 'a', 'Tm', '?Hmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk']]\n",
    "\n",
    "nol_y = nol_df[[ 'AM?', 'IM?', 'SS?']]\n",
    "\n",
    "nol_X_train, nol_X_test, nol_y_train, nol_y_test = train_test_split(nol_X, nol_y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we observe that most outlier alloys contain Al, B, Mg and Cd.\n",
    "\n",
    "### Preparing training and testing sets\n",
    "\n",
    "After observing that many alloys of 4 or more metals show significant differences in their properties as compared most alloys of lesser no. of constituents. Thus, we want alloys of all no. of metals to be equally distributed in our training, validation and test sets. To acheive this, we separate alloys into binary, ternary, quaternary and higher order, shuffle them and distribute them to the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_file = 'AM_IM_SS_New_AM9.csv'\n",
    "num_of_feature = 9\n",
    "\n",
    "binary = np.zeros([163, num_of_feature+3])\n",
    "ternary = np.zeros([120, num_of_feature+3])\n",
    "quaternary = np.zeros([89, num_of_feature+3])\n",
    "highorder = np.zeros([229, num_of_feature+3])\n",
    "\n",
    "#iterator variable for each class\n",
    "c2 = 0\n",
    "c3 = 0\n",
    "c4 = 0\n",
    "c5 = 0\n",
    "file = open(data_file, mode='r')\n",
    "lines = file.readlines()\n",
    "for i, l in enumerate(lines):\n",
    "    data = l.split(',')\n",
    "    if int(data[1]) == 2:\n",
    "        for j, d in enumerate(data):\n",
    "            if j >= 4 and j <= num_of_feature+3:\n",
    "                binary[c2][j-4] = float(d)\n",
    "            elif j >= num_of_feature+6 and j<= num_of_feature+8:\n",
    "                binary[c2][j-6] = int(d)\n",
    "        c2+=1\n",
    "    elif int(data[1]) == 3:\n",
    "        for j, d in enumerate(data):\n",
    "            if j >= 4 and j <= num_of_feature+3:\n",
    "                ternary[c3][j-4] = float(d)\n",
    "            elif j >= num_of_feature+6 and j<= num_of_feature+8:\n",
    "                ternary[c3][j-6] = int(d)\n",
    "        c3+=1\n",
    "    elif int(data[1]) == 4:\n",
    "        for j, d in enumerate(data):\n",
    "            if j >= 4 and j <= num_of_feature+3:\n",
    "                quaternary[c4][j-4] = float(d)\n",
    "            elif j >= num_of_feature+6 and j<= num_of_feature+8:\n",
    "                quaternary[c4][j-6] = int(d)\n",
    "        c4+=1\n",
    "    else:\n",
    "        for j, d in enumerate(data):\n",
    "            if j >= 4 and j <= num_of_feature+3:\n",
    "                highorder[c5][j-4] = float(d)\n",
    "            elif j >= num_of_feature+6 and j<= num_of_feature+8:\n",
    "                highorder[c5][j-6] = int(d)\n",
    "        c5+=1\n",
    "\n",
    "for n in range(0, 10):\n",
    "    np.random.shuffle(binary)\n",
    "    np.random.shuffle(ternary)\n",
    "    np.random.shuffle(quaternary)\n",
    "    np.random.shuffle(highorder)\n",
    "    \n",
    "train_data = np.concatenate((binary[:114], ternary[:84], quaternary[:62], highorder[:160]))\n",
    "test_data = np.concatenate((binary[114:139], ternary[84:102], quaternary[62:76], highorder[160:194]))\n",
    "val_data = np.concatenate((binary[139:], ternary[102:], quaternary[76:], highorder[194:]))\n",
    "x_train = train_data[:, :num_of_feature]\n",
    "y_train_AM = np.squeeze(train_data[:, num_of_feature: num_of_feature+1])\n",
    "y_train_IM = np.squeeze(train_data[:, num_of_feature+1: num_of_feature+2])\n",
    "y_train_SS = np.squeeze(train_data[:, num_of_feature+2: num_of_feature+3])\n",
    "x_val = val_data[:, :num_of_feature]\n",
    "y_val_AM = np.squeeze(val_data[:, num_of_feature: num_of_feature+1])\n",
    "y_val_IM = np.squeeze(val_data[:, num_of_feature+1: num_of_feature+2])\n",
    "y_val_SS = np.squeeze(val_data[:, num_of_feature+2: num_of_feature+3])\n",
    "x_test = test_data[:, :num_of_feature]\n",
    "y_test_AM = np.squeeze(test_data[:, num_of_feature: num_of_feature+1])\n",
    "y_test_IM = np.squeeze(test_data[:, num_of_feature+1: num_of_feature+2])\n",
    "y_test_SS = np.squeeze(test_data[:, num_of_feature+2: num_of_feature+3])\n",
    "y_train = np.column_stack((y_train_AM, y_train_IM, y_train_SS))\n",
    "y_val = np.column_stack((y_val_AM, y_val_IM, y_val_SS))\n",
    "y_test = np.column_stack((y_test_AM, y_test_IM, y_test_SS))\n",
    "\n",
    "\n",
    "#print(np.unique(y_train, axis=0))\n",
    "#AM= 1, IM=2, SS=3, IM+SS=4, AM+IM=5\n",
    "\n",
    "#print(y_train)\n",
    "\n",
    "\n",
    "def encode(lis):\n",
    "    if lis==str('[1. 0. 0.]'):\n",
    "        return 'AM'\n",
    "    if lis==str('[0. 1. 0.]'):\n",
    "        return 'IM'\n",
    "    if lis==str('[0. 0. 1.]'):\n",
    "        return 'SS'\n",
    "    if lis==str('[0. 1. 1.]'):\n",
    "        return 'IM+SS'\n",
    "    if lis==str('[1. 1. 0.]'):\n",
    "        return 'AM+IM'\n",
    "y_train = [encode(str(i)) for i in y_train]\n",
    "y_val = [encode(str(i)) for i in y_val]\n",
    "y_test = [encode(str(i)) for i in y_test]\n",
    "nol_y_train = [encode(str(i)) for i in nol_y_train.to_numpy()]\n",
    "nol_y_test = [encode(str(i)) for i in nol_y_test.to_numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling our Data\n",
    "\n",
    "We use scikit learn Standard Scalar for scaling our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = pd.DataFrame(data=x_train, columns=['a', 'Tm', 'åHmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk'])\n",
    "X_val = pd.DataFrame(data=x_val, columns=['a', 'Tm', 'åHmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk'])\n",
    "X_test = pd.DataFrame(data=x_test, columns=['a', 'Tm', 'åHmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk'])\n",
    "\n",
    "Y_train = pd.DataFrame(data=y_train)\n",
    "Y_val = pd.DataFrame(data=y_val)\n",
    "Y_test = pd.DataFrame(data=y_test)\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_val = sc_x.transform(X_val)\n",
    "X_test = sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using vaious classifiers\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.88      0.82      0.85        28\n",
      "       AM+IM       0.00      0.00      0.00         0\n",
      "          IM       0.70      0.88      0.78        32\n",
      "       IM+SS       0.00      0.00      0.00        11\n",
      "          SS       0.59      0.68      0.63        19\n",
      "\n",
      "    accuracy                           0.71        90\n",
      "   macro avg       0.44      0.48      0.45        90\n",
      "weighted avg       0.65      0.71      0.68        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'Tm', '?Hmix', 'Elec_nega', 'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk']\n",
      "Feature importances:\n",
      "a 0.11172024083019148\n",
      "Tm 0.12279188802036763\n",
      "?Hmix 0.15709226470411272\n",
      "Elec_nega 0.09371596767221085\n",
      "D_elec_nega 0.13749164565583838\n",
      "VEC 0.07204578701913218\n",
      "d_VEC 0.11106228234274387\n",
      "BulkModulus 0.09943278715902802\n",
      "D_Bulk 0.09464713659637496\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "#indices = np.argsort(importances)[::-1]\n",
    "features = [ 'a', 'Tm', '?Hmix', 'Elec_nega',\n",
    "       'D_elec_nega', 'VEC', 'd_VEC', 'BulkModulus', 'D_Bulk']\n",
    "# Print the feature ranking\n",
    "#features = features[indices]\n",
    "print(features)\n",
    "print(\"Feature importances:\")\n",
    "\n",
    "for i,j in zip(features, importances):\n",
    "    print(i,j)\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances,\n",
    "        color=\"r\", yerr=std, align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), features)\n",
    "\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier gives us the importance of each feature. According to this classifier, Enthalpy of mixing is the most important feature and VEC is the least important feature.\n",
    "\n",
    "### K-nearest Neighbor (KNN)\n",
    "\n",
    "This algorithm classifies data points based on clustering of nearest neighbors by Euclidean Distance or any other meaasure for distance between two data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.84      0.96      0.90        28\n",
      "          IM       1.00      0.72      0.84        32\n",
      "       IM+SS       0.60      0.55      0.57        11\n",
      "          SS       0.64      0.84      0.73        19\n",
      "\n",
      "    accuracy                           0.80        90\n",
      "   macro avg       0.77      0.77      0.76        90\n",
      "weighted avg       0.83      0.80      0.80        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 4,\n",
    "                                  weights='uniform') #wights='distance' gives 90% acc.\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "\n",
    "Support Vector Machines classifiy data points by learning a hyperplane distinguising the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.96      0.93      0.95        28\n",
      "          IM       0.93      0.88      0.90        32\n",
      "       IM+SS       1.00      0.18      0.31        11\n",
      "          SS       0.61      1.00      0.76        19\n",
      "\n",
      "    accuracy                           0.83        90\n",
      "   macro avg       0.88      0.75      0.73        90\n",
      "weighted avg       0.88      0.83      0.81        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier =SVC(kernel = 'rbf', random_state = 0) #rbf givees max. acc\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gausian Naive Bayes\n",
    "\n",
    "Naiva Bayes Algorithm calculates probability for each class, and computes the conditional probability of each input for its corresponding class. The Gaussian Naive Bayes assumes that the data points are distributed normally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5888888888888889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.86      0.68      0.76        28\n",
      "       AM+IM       0.00      0.00      0.00         0\n",
      "          IM       0.86      0.75      0.80        32\n",
      "       IM+SS       0.28      0.73      0.40        11\n",
      "          SS       0.22      0.11      0.14        19\n",
      "\n",
      "    accuracy                           0.59        90\n",
      "   macro avg       0.44      0.45      0.42        90\n",
      "weighted avg       0.65      0.59      0.60        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier =GaussianNB()\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifiers\n",
    "\n",
    "These classifiers create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.93      0.93      0.93        28\n",
      "          IM       0.86      0.75      0.80        32\n",
      "       IM+SS       0.62      0.45      0.53        11\n",
      "          SS       0.58      0.79      0.67        19\n",
      "\n",
      "    accuracy                           0.78        90\n",
      "   macro avg       0.75      0.73      0.73        90\n",
      "weighted avg       0.79      0.78      0.78        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier =DecisionTreeClassifier(criterion = 'gini', random_state = 0)#gini better than entropy\n",
    "\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD classifier\n",
    "\n",
    "These classifiers optimize using the Stochastic Gradient Descent rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.86      0.68      0.76        28\n",
      "          IM       0.66      0.66      0.66        32\n",
      "       IM+SS       0.27      0.36      0.31        11\n",
      "          SS       0.48      0.53      0.50        19\n",
      "\n",
      "    accuracy                           0.60        90\n",
      "   macro avg       0.57      0.56      0.56        90\n",
      "weighted avg       0.64      0.60      0.61        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier = SGDClassifier(max_iter=1000, tol=1e-3)#accuracy peaks for more estimators\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies for all the models above are summarized below.\n",
    "\n",
    "| Model                      | Accuracy    |\n",
    "| -------------------------- | ----------- |\n",
    "| Logistic Regression        | 72.2%       |\n",
    "| K-nearest Neighbor (KNN)   | 76.6%       |\n",
    "| Gaussian Naive Bayes (KNN) | 60%         |\n",
    "| Support Vector Classifier  | 75.5%       |\n",
    "| Decision Tree Classifiers  | 78.8%       |\n",
    "| SGD Classifier             | 60.0%       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to ensemble methods for better accuracy\n",
    "\n",
    "## Ensemble methods\n",
    "### Bagging Classifier\n",
    "\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5777777777777777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.60      0.56      0.58        27\n",
      "       AM+IM       0.00      0.00      0.00         3\n",
      "          IM       0.79      0.62      0.70        37\n",
      "       IM+SS       0.00      0.00      0.00         7\n",
      "          SS       0.44      0.88      0.58        16\n",
      "\n",
      "    accuracy                           0.58        90\n",
      "   macro avg       0.37      0.41      0.37        90\n",
      "weighted avg       0.58      0.58      0.56        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_pred = classifier.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       1.00      0.80      0.89        25\n",
      "       AM+IM       0.00      0.00      0.00         3\n",
      "          IM       0.85      0.92      0.89        38\n",
      "       IM+SS       0.67      0.33      0.44         6\n",
      "          SS       0.72      1.00      0.84        18\n",
      "\n",
      "    accuracy                           0.83        90\n",
      "   macro avg       0.65      0.61      0.61        90\n",
      "weighted avg       0.83      0.83      0.82        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100, max_depth=None, random_state=0)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifer\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.32      0.44      0.37        25\n",
      "       AM+IM       0.00      0.00      0.00         3\n",
      "          IM       0.81      0.55      0.66        38\n",
      "       IM+SS       0.00      0.00      0.00         6\n",
      "          SS       0.20      0.33      0.25        18\n",
      "\n",
      "    accuracy                           0.42        90\n",
      "   macro avg       0.27      0.27      0.26        90\n",
      "weighted avg       0.47      0.42      0.43        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Bootsing Classifier\n",
    "\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.91      0.84      0.87        25\n",
      "       AM+IM       0.00      0.00      0.00         3\n",
      "          IM       0.87      0.87      0.87        38\n",
      "       IM+SS       0.38      0.50      0.43         6\n",
      "          SS       0.71      0.83      0.77        18\n",
      "\n",
      "    accuracy                           0.80        90\n",
      "   macro avg       0.57      0.61      0.59        90\n",
      "weighted avg       0.79      0.80      0.79        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "\n",
    "The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.84      0.84      0.84        25\n",
      "       AM+IM       0.00      0.00      0.00         3\n",
      "          IM       0.92      0.89      0.91        38\n",
      "       IM+SS       0.75      0.50      0.60         6\n",
      "          SS       0.70      0.89      0.78        18\n",
      "\n",
      "    accuracy                           0.82        90\n",
      "   macro avg       0.64      0.62      0.63        90\n",
      "weighted avg       0.81      0.82      0.81        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = LogisticRegression()\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)])\n",
    "eclf.fit(X_train, Y_train)\n",
    "y_pred = eclf.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Model Classification\n",
    "\n",
    "It uses a meta-learning algorithm to learn how to best combine the predictions from two or more base machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:672: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.91      0.80      0.85        25\n",
      "       AM+IM       0.00      0.00      0.00         3\n",
      "          IM       0.89      0.87      0.88        38\n",
      "       IM+SS       0.29      0.33      0.31         6\n",
      "          SS       0.71      0.94      0.81        18\n",
      "\n",
      "    accuracy                           0.80        90\n",
      "   macro avg       0.56      0.59      0.57        90\n",
      "weighted avg       0.79      0.80      0.79        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = LogisticRegression()\n",
    "\n",
    "estimators = [('clf1', clf1),\n",
    "             ('clf2', clf2),\n",
    "             ('clf3', clf3)]\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=GradientBoostingClassifier(n_estimators=100))\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "print(accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies for all the models above are summarized below.\n",
    "\n",
    "| Model                        | Accuracy    |\n",
    "| --------------------------   | ----------- |\n",
    "| Bagging Classifier           | 74.4%       |\n",
    "| Random Forests Classifier    | 83.3%       |\n",
    "| AdaBoost Classifer           | 42.2%       |\n",
    "| Gradient Boosing Classifier  | 80.0%       |\n",
    "| Voting Classifier            | 82.2%       |\n",
    "| Stacked Model Classification | 80.0%       |\n",
    "\n",
    "We observe that these ensemble methods give better accuracies ovarall, AdaBoost Classifier being an exception.\n",
    "\n",
    "## Deep Learning Method\n",
    "\n",
    "We use a 3-layer deep dense Neural Network with RelU activation on the hidden layer andn Sigmoid on the output layer. All the hidden layers had a dropout rate of 0.5. Categorical crossentropy loss was used in the Adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "420/420 [==============================] - 1s 1ms/step - loss: 11.6664 - acc: 0.2762 \n",
      "Epoch 2/150\n",
      "420/420 [==============================] - 0s 103us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 3/150\n",
      "420/420 [==============================] - 0s 147us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 4/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 5/150\n",
      "420/420 [==============================] - 0s 122us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 6/150\n",
      "420/420 [==============================] - 0s 112us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 7/150\n",
      "420/420 [==============================] - 0s 137us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 8/150\n",
      "420/420 [==============================] - 0s 133us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 9/150\n",
      "420/420 [==============================] - 0s 124us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 10/150\n",
      "420/420 [==============================] - 0s 123us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 11/150\n",
      "420/420 [==============================] - 0s 92us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 12/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 13/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 14/150\n",
      "420/420 [==============================] - 0s 116us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 15/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 16/150\n",
      "420/420 [==============================] - 0s 128us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 17/150\n",
      "420/420 [==============================] - 0s 127us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 18/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 19/150\n",
      "420/420 [==============================] - 0s 112us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 20/150\n",
      "420/420 [==============================] - 0s 116us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 21/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 22/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 23/150\n",
      "420/420 [==============================] - 0s 123us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 24/150\n",
      "420/420 [==============================] - 0s 109us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 25/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 26/150\n",
      "420/420 [==============================] - 0s 116us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 27/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 28/150\n",
      "420/420 [==============================] - 0s 120us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 29/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 30/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 31/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 32/150\n",
      "420/420 [==============================] - 0s 138us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 33/150\n",
      "420/420 [==============================] - 0s 129us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 34/150\n",
      "420/420 [==============================] - 0s 123us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 35/150\n",
      "420/420 [==============================] - 0s 128us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 36/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 37/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 38/150\n",
      "420/420 [==============================] - 0s 142us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 39/150\n",
      "420/420 [==============================] - 0s 135us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 40/150\n",
      "420/420 [==============================] - 0s 123us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 41/150\n",
      "420/420 [==============================] - 0s 129us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 42/150\n",
      "420/420 [==============================] - 0s 135us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 43/150\n",
      "420/420 [==============================] - 0s 103us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 44/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 45/150\n",
      "420/420 [==============================] - 0s 129us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 46/150\n",
      "420/420 [==============================] - 0s 126us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 47/150\n",
      "420/420 [==============================] - 0s 123us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 48/150\n",
      "420/420 [==============================] - 0s 132us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 49/150\n",
      "420/420 [==============================] - 0s 116us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 50/150\n",
      "420/420 [==============================] - 0s 126us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 51/150\n",
      "420/420 [==============================] - 0s 122us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 52/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 53/150\n",
      "420/420 [==============================] - 0s 112us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 54/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 55/150\n",
      "420/420 [==============================] - 0s 140us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 56/150\n",
      "420/420 [==============================] - 0s 126us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 57/150\n",
      "420/420 [==============================] - 0s 105us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 58/150\n",
      "420/420 [==============================] - 0s 101us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 59/150\n",
      "420/420 [==============================] - 0s 130us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 60/150\n",
      "420/420 [==============================] - 0s 122us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 61/150\n",
      "420/420 [==============================] - 0s 104us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 62/150\n",
      "420/420 [==============================] - 0s 129us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 63/150\n",
      "420/420 [==============================] - 0s 102us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 64/150\n",
      "420/420 [==============================] - 0s 120us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 65/150\n",
      "420/420 [==============================] - 0s 123us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 66/150\n",
      "420/420 [==============================] - 0s 102us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 67/150\n",
      "420/420 [==============================] - 0s 122us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 68/150\n",
      "420/420 [==============================] - 0s 117us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 69/150\n",
      "420/420 [==============================] - 0s 120us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 70/150\n",
      "420/420 [==============================] - 0s 124us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 71/150\n",
      "420/420 [==============================] - 0s 108us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 72/150\n",
      "420/420 [==============================] - 0s 120us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 73/150\n",
      "420/420 [==============================] - 0s 120us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 74/150\n",
      "420/420 [==============================] - 0s 105us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 75/150\n",
      "420/420 [==============================] - 0s 128us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 76/150\n",
      "420/420 [==============================] - ETA: 0s - loss: 12.8945 - acc: 0.20 - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 77/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 78/150\n",
      "420/420 [==============================] - 0s 115us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 79/150\n",
      "420/420 [==============================] - 0s 117us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 80/150\n",
      "420/420 [==============================] - 0s 135us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 81/150\n",
      "420/420 [==============================] - 0s 103us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420/420 [==============================] - 0s 107us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 83/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 84/150\n",
      "420/420 [==============================] - 0s 115us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 85/150\n",
      "420/420 [==============================] - 0s 117us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 86/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 87/150\n",
      "420/420 [==============================] - 0s 116us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 88/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 89/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 90/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 91/150\n",
      "420/420 [==============================] - 0s 100us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 92/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 93/150\n",
      "420/420 [==============================] - 0s 108us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 94/150\n",
      "420/420 [==============================] - 0s 110us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 95/150\n",
      "420/420 [==============================] - 0s 106us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 96/150\n",
      "420/420 [==============================] - 0s 120us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 97/150\n",
      "420/420 [==============================] - 0s 117us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 98/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 99/150\n",
      "420/420 [==============================] - 0s 117us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 100/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 101/150\n",
      "420/420 [==============================] - 0s 115us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 102/150\n",
      "420/420 [==============================] - 0s 116us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 103/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 104/150\n",
      "420/420 [==============================] - 0s 106us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 105/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 106/150\n",
      "420/420 [==============================] - 0s 118us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 107/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 108/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 109/150\n",
      "420/420 [==============================] - 0s 132us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 110/150\n",
      "420/420 [==============================] - 0s 173us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 111/150\n",
      "420/420 [==============================] - 0s 155us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 112/150\n",
      "420/420 [==============================] - 0s 152us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 113/150\n",
      "420/420 [==============================] - 0s 156us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 114/150\n",
      "420/420 [==============================] - 0s 160us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 115/150\n",
      "420/420 [==============================] - 0s 162us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 116/150\n",
      "420/420 [==============================] - 0s 152us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 117/150\n",
      "420/420 [==============================] - 0s 152us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 118/150\n",
      "420/420 [==============================] - 0s 189us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 119/150\n",
      "420/420 [==============================] - 0s 161us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 120/150\n",
      "420/420 [==============================] - 0s 128us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 121/150\n",
      "420/420 [==============================] - 0s 159us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 122/150\n",
      "420/420 [==============================] - 0s 159us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 123/150\n",
      "420/420 [==============================] - 0s 188us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 124/150\n",
      "420/420 [==============================] - 0s 152us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 125/150\n",
      "420/420 [==============================] - 0s 121us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 126/150\n",
      "420/420 [==============================] - 0s 112us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 127/150\n",
      "420/420 [==============================] - 0s 111us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 128/150\n",
      "420/420 [==============================] - 0s 110us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 129/150\n",
      "420/420 [==============================] - 0s 111us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 130/150\n",
      "420/420 [==============================] - 0s 113us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 131/150\n",
      "420/420 [==============================] - 0s 112us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 132/150\n",
      "420/420 [==============================] - 0s 129us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 133/150\n",
      "420/420 [==============================] - 0s 109us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 134/150\n",
      "420/420 [==============================] - 0s 97us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 135/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 136/150\n",
      "420/420 [==============================] - 0s 117us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 137/150\n",
      "420/420 [==============================] - 0s 111us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 138/150\n",
      "420/420 [==============================] - 0s 106us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 139/150\n",
      "420/420 [==============================] - 0s 107us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 140/150\n",
      "420/420 [==============================] - 0s 114us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 141/150\n",
      "420/420 [==============================] - 0s 139us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 142/150\n",
      "420/420 [==============================] - 0s 133us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 143/150\n",
      "420/420 [==============================] - 0s 138us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 144/150\n",
      "420/420 [==============================] - 0s 136us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 145/150\n",
      "420/420 [==============================] - 0s 144us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 146/150\n",
      "420/420 [==============================] - 0s 144us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 147/150\n",
      "420/420 [==============================] - 0s 136us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 148/150\n",
      "420/420 [==============================] - 0s 128us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 149/150\n",
      "420/420 [==============================] - 0s 138us/step - loss: 11.6664 - acc: 0.2762\n",
      "Epoch 150/150\n",
      "420/420 [==============================] - 0s 119us/step - loss: 11.6664 - acc: 0.2762\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.30\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=9, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(12,  activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(12,  activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_val = ohe.transform(np.array(y_val).reshape(-1, 1)).toarray()\n",
    "# fit the keras model on the dataset\n",
    "#print(y_train)\n",
    "model.fit(x_train, y_train, epochs=150, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(x_val, y_val)\n",
    "print('Accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the Neural Network performs badly on this data. This maybe because the dataset is too small to generalize.\n",
    "\n",
    "## Test Accuracy\n",
    "\n",
    "We use our best performing model(Random Forests Clasifier with Extra trees) to predict on the test set to get final test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8681318681318682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.92      0.88      0.90        25\n",
      "          IM       0.91      0.97      0.94        40\n",
      "       IM+SS       0.43      0.60      0.50         5\n",
      "          SS       0.88      0.71      0.79        21\n",
      "\n",
      "    accuracy                           0.87        91\n",
      "   macro avg       0.78      0.79      0.78        91\n",
      "weighted avg       0.88      0.87      0.87        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100, max_depth=None, random_state=0)\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the final test accuracy was 86.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also try training this model after removing all outliers found by the LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8319327731092437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AM       0.97      0.88      0.92        40\n",
      "       AM+IM       0.00      0.00      0.00         2\n",
      "          IM       0.89      0.89      0.89        38\n",
      "       IM+SS       0.45      0.56      0.50         9\n",
      "          SS       0.74      0.83      0.78        30\n",
      "\n",
      "    accuracy                           0.83       119\n",
      "   macro avg       0.61      0.63      0.62       119\n",
      "weighted avg       0.83      0.83      0.83       119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nirnay\\Anaconda3\\envs\\keras\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100, max_depth=None, random_state=0)\n",
    "clf.fit(nol_X_train, nol_y_train)\n",
    "y_pred = clf.predict(nol_X_test)\n",
    "print(accuracy_score(nol_y_test, y_pred))\n",
    "print(classification_report(nol_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a 3% decrease in classification accuracy. The causes for this are yet to be investigated.\n",
    "\n",
    "## References\n",
    "- [Machine-learning phase prediction of high-entropy alloys](https://doi.org/10.1016/j.actamat.2019.03.012)\n",
    "- [Machine learning guided appraisal and exploration of phase design for high entropy alloys](https://www.nature.com/articles/s41524-019-0265-1)\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
